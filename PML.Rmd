---
title: "Practical Machine Learning - Peer Assessment"
author: "Vaibhav Joshi"
date: "5 October 2017"
output: html_document
---
## Summary

This submission analyses Human Activity Recognition (HAR) data to assess the users' form when performing weight lifting repetitions.  This predicts the manner in which the participants did the exercise. I refer to the "classe" variable in the training set with freedom to use any other variable as predictors.

## Assessment Criteria

You should create a report describing:

1. how you built your model

2. how you used cross validation

3. what you think the expected out of sample error is, and 

4. why you made the choices you did. 


5. You will also use your prediction model to predict 20 different test cases.

### Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here:](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset). 

## Solution choices

Given the width of the dataset (~60 variables post clean) and intention to make a class prediction, I decided to implement a random forests model. 

I split the training dataset 60/40 to provide train/validate sets. 

The source documentation (http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har) states that the "Incorrectly Classified Instances" is 0.5856%.  Given that the test set is only 20 questions, we can expect not to have any errors.

### Data 

The dataset has 5 classes (sitting-down, standing-up, standing, walking, and sitting) collected on 8 hours of activities of healthy subjects. Training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

### Set the work environment and knitr options

Make it nice and readbility for all you nice markers out there.

```{r setoptions}
rm(list=ls(all=TRUE)) #start with empty workspace
startTime <- Sys.time()

library(knitr, quietly = TRUE)
opts_chunk$set(echo = TRUE, cache= TRUE, results = 'hold')

```

### Load libraries & set seed for reproducibility

```{r}
library(caret)
library(randomForest, quietly = TRUE)
set.seed(1)
```

## Load data into memory (check before redownloading)

```{r}
trUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
ttUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile <- "./data/pml-training.csv"
testFile  <- "./data/pml-testing.csv"
if (!file.exists("./data")) {
  dir.create("./data")
}
if (!file.exists(trainFile)) {
  download.file(trUrl, destfile=trainFile)
}
if (!file.exists(testFile)) {
  download.file(ttUrl, destfile=testFile)
}
trainRaw <- read.csv("./data/pml-training.csv",header=T,sep=",",na.strings=c("NA","")) #skip header + clean
testRaw <- read.csv("./data/pml-testing.csv",header=T,sep=",",na.strings=c("NA",""))   #skip header + clean

keep <- c((colSums(is.na(trainRaw[,-ncol(trainRaw)])) == 0))  #remove columns without data, note removed columns
trainRaw <- trainRaw[,keep]                         

inTrain <- createDataPartition(y=trainRaw$classe, p=0.60, list=FALSE) #split 60/40 train/validate
train <- trainRaw[inTrain,]
valid <- trainRaw[-inTrain,]
```

## Modelling

Let's begin by: 

1. seeing what this looks like

2. evaluate the importance of the variables

3. evaluate using a confusion matrix using validation set

```{r}
plot(trainRaw$classe, main="Distribution", xlab="Class of Activity (Classe)", ylab="Frequency")
model <- randomForest(classe~.,data=train)
model
importance(model)
confusionMatrix(predict(model,newdata=valid[,-ncol(valid)]),valid$classe)
```

## Prediction (Step 5)

Final step: let's see what we get!
```{r}
testRaw <- testRaw[ ,keep] # retain same columns as training
testRaw <- testRaw[,-ncol(testRaw)] # Remove the "problem ID" column
testing  <- rbind(train[100, -60] , testRaw) 
row.names(testing) <- c(100, 1:20) #ensure match
predictions <- predict(model,newdata=testing[-1,])
predictions
```